{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Predicción de Riesgo de Enfermedad Cardíaca: Tarea de Regresión Logística\n",
                "\n",
                "## Contexto Introductorio\n",
                "Las enfermedades cardíacas son la principal causa de muerte en el mundo. Los modelos predictivos como la regresión logística pueden permitir la identificación temprana de pacientes en riesgo mediante el análisis de características clínicas. En esta tarea, implementaremos la regresión logística en el conjunto de datos de enfermedades cardíacas (muestras de pacientes con características como edad, colesterol, presión arterial).\n",
                "\n",
                "## Instrucciones\n",
                "Completar en este cuaderno Jupyter, implementando funciones de la teoría de clase (sigmoide, costo, GD). Usar NumPy, Pandas y Matplotlib.\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Paso 1: Cargar y Preparar el Dataset"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import numpy as np\n",
                "import pandas as pd\n",
                "import matplotlib.pyplot as plt\n",
                "\n",
                "# Configuración de gráficos\n",
                "%matplotlib inline\n",
                "plt.style.use('ggplot')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 1.1 Cargar Datos\n",
                "Cargaremos el dataset desde la carpeta `data/`."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "\n",
                "data_path = 'data/'\n",
                "files = [f for f in os.listdir(data_path) if f.endswith('.csv')]\n",
                "if not files:\n",
                "    raise FileNotFoundError(\"No se encontró el archivo CSV en la carpeta data/\")\n",
                "csv_file = files[0]\n",
                "print(f\"Cargando archivo: {csv_file}\")\n",
                "\n",
                "df = pd.read_csv(os.path.join(data_path, csv_file))\n",
                "df.head()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 1.2 Preprocesamiento y EDA\n",
                "Convertiremos la columna objetivo `Heart Disease` a binaria (1 para Presence, 0 para Absence) y realizaremos un análisis exploratorio básico."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Binarizar la columna objetivo\n",
                "df['Heart Disease'] = df['Heart Disease'].map({'Presence': 1, 'Absence': 0})\n",
                "\n",
                "# Verificar valores nulos\n",
                "print(\"Valores nulos por columna:\")\n",
                "print(df.isnull().sum())\n",
                "\n",
                "# Estadísticas descriptivas\n",
                "df.describe()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Distribución de clases\n",
                "plt.figure(figsize=(6, 4))\n",
                "df['Heart Disease'].value_counts().plot(kind='bar', color=['skyblue', 'salmon'])\n",
                "plt.title('Distribución de la Enfermedad Cardíaca (0=Ausencia, 1=Presencia)')\n",
                "plt.xlabel('Clase')\n",
                "plt.ylabel('Cantidad')\n",
                "plt.xticks(ticks=[0, 1], labels=['Ausencia', 'Presencia'], rotation=0)\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 1.3 Selección de Características y Normalización\n",
                "Utilizaremos una selección de características indicadas y normalizaremos los datos."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Selección de características (features) y objetivo (target)\n",
                "# Seleccionamos >= 6 características como se solicita\n",
                "features = ['Age', 'Cholesterol', 'BP', 'Max HR', 'ST depression', 'Number of vessels fluro']\n",
                "target = 'Heart Disease'\n",
                "\n",
                "X = df[features].values\n",
                "y = df[target].values\n",
                "\n",
                "# Normalización (Min-Max Scaling manual o usando librerías si se prefiere, aquí manual para ejercicio)\n",
                "def normalize(X):\n",
                "    min_val = np.min(X, axis=0)\n",
                "    max_val = np.max(X, axis=0)\n",
                "    return (X - min_val) / (max_val - min_val), min_val, max_val\n",
                "\n",
                "X_norm, min_val, max_val = normalize(X)\n",
                "\n",
                "print(\"Forma de X:\", X_norm.shape)\n",
                "print(\"Forma de y:\", y.shape)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 1.4 División Train/Test\n",
                "Dividiremos los datos en 70% entrenamiento y 30% prueba de manera estratificada."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from sklearn.model_selection import train_test_split\n",
                "\n",
                "X_train, X_test, y_train, y_test = train_test_split(X_norm, y, test_size=0.3, stratify=y, random_state=42)\n",
                "\n",
                "print(f\"Train set: {X_train.shape[0]} muestras\")\n",
                "print(f\"Test set: {X_test.shape[0]} muestras\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Paso 2: Implementar Regresión Logística Básica\n",
                "Implementaremos las funciones `sigmoid`, `cost` y `gradient_descent` desde cero."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def sigmoid(z):\n",
                "    \"\"\"\n",
                "    Calcula la sigmoide de z\n",
                "    \"\"\"\n",
                "    return 1 / (1 + np.exp(-z))\n",
                "\n",
                "def compute_cost(X, y, w, b):\n",
                "    \"\"\"\n",
                "    Calcula la función de costo (Binary Cross-Entropy)\n",
                "    \"\"\"\n",
                "    m = X.shape[0]\n",
                "    z = np.dot(X, w) + b\n",
                "    f_wb = sigmoid(z)\n",
                "    \n",
                "    # Evitar log(0) añadiendo un epsilon pequeño\n",
                "    epsilon = 1e-15\n",
                "    cost = -1/m * np.sum(y * np.log(f_wb + epsilon) + (1 - y) * np.log(1 - f_wb + epsilon))\n",
                "    return cost\n",
                "\n",
                "def compute_gradient(X, y, w, b):\n",
                "    \"\"\"\n",
                "    Calcula el gradiente para el descenso de gradiente\n",
                "    \"\"\"\n",
                "    m, n = X.shape\n",
                "    z = np.dot(X, w) + b\n",
                "    f_wb = sigmoid(z)\n",
                "    \n",
                "    err = f_wb - y\n",
                "    dj_dw = (1/m) * np.dot(X.top, err) if hasattr(X, 'top') else (1/m) * np.dot(X.T, err)\n",
                "    dj_db = (1/m) * np.sum(err)\n",
                "    \n",
                "    return dj_dw, dj_db\n",
                "\n",
                "def gradient_descent(X, y, w_in, b_in, alpha, num_iters):\n",
                "    \"\"\"\n",
                "    Realiza el descenso de gradiente para aprender w y b\n",
                "    \"\"\"\n",
                "    m = X.shape[0]\n",
                "    J_history = []\n",
                "    w = copy.deepcopy(w_in)\n",
                "    b = b_in\n",
                "    \n",
                "    for i in range(num_iters):\n",
                "        dj_dw, dj_db = compute_gradient(X, y, w, b)\n",
                "        \n",
                "        w = w - alpha * dj_dw\n",
                "        b = b - alpha * dj_db\n",
                "        \n",
                "        if i % 100 == 0:\n",
                "            cost = compute_cost(X, y, w, b)\n",
                "            J_history.append(cost)\n",
                "            # print(f\"Iteración {i:4d}: Costo {cost:8.2f}\") # Opcional: imprimir progreso\n",
                "            \n",
                "    return w, b, J_history"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 2.1 Entrenamiento del Modelo\n",
                "Entrenaremos el modelo utilizando el conjunto de entrenamiento completo."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import copy\n",
                "\n",
                "# Inicialización de parámetros\n",
                "m, n = X_train.shape\n",
                "initial_w = np.zeros(n)\n",
                "initial_b = 0.\n",
                "iterations = 2000\n",
                "alpha = 0.01\n",
                "\n",
                "print(\"Iniciando entrenamiento...\")\n",
                "w_final, b_final, J_hist = gradient_descent(X_train, y_train, initial_w, initial_b, alpha, iterations)\n",
                "print(f\"Entrenamiento completado. w: {w_final}, b: {b_final:0.2f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 2.2 Gráfica de Costo vs Iteraciones"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "plt.plot(J_hist)\n",
                "plt.title(\"Costo vs Iteraciones\")\n",
                "plt.ylabel('Costo')\n",
                "plt.xlabel('Iteraciones (x100)')\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 2.3 Evaluación del Modelo\n",
                "Evaluaremos el modelo en los conjuntos de entrenamiento y prueba."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def predict(X, w, b, threshold=0.5):\n",
                "    z = np.dot(X, w) + b\n",
                "    p = sigmoid(z)\n",
                "    return (p >= threshold).astype(int)\n",
                "\n",
                "def evaluate_metrics(y_true, y_pred):\n",
                "    tp = np.sum((y_true == 1) & (y_pred == 1))\n",
                "    tn = np.sum((y_true == 0) & (y_pred == 0))\n",
                "    fp = np.sum((y_true == 0) & (y_pred == 1))\n",
                "    fn = np.sum((y_true == 1) & (y_pred == 0))\n",
                "    \n",
                "    accuracy = (tp + tn) / len(y_true)\n",
                "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
                "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
                "    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
                "    \n",
                "    return accuracy, precision, recall, f1\n",
                "\n",
                "# Predicciones\n",
                "y_train_pred = predict(X_train, w_final, b_final)\n",
                "y_test_pred = predict(X_test, w_final, b_final)\n",
                "\n",
                "# Métricas\n",
                "metrics_train = evaluate_metrics(y_train, y_train_pred)\n",
                "metrics_test = evaluate_metrics(y_test, y_test_pred)\n",
                "\n",
                "results_df = pd.DataFrame([metrics_train, metrics_test], \n",
                "                          columns=['Accuracy', 'Precision', 'Recall', 'F1 Score'], \n",
                "                          index=['Train', 'Test'])\n",
                "results_df"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.5"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}